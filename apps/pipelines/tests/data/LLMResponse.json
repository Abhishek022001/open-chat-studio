{
  "description": "Calls an LLM with the given input",
  "properties": {
    "llm_provider_id": {
      "title": "LLM Model",
      "type": "integer",
      "ui:widget": "llm_provider_model"
    },
    "llm_provider_model_id": {
      "title": "Llm Provider Model Id",
      "type": "integer",
      "ui:widget": "none"
    },
    "llm_temperature": {
      "default": 0.7,
      "maximum": 2.0,
      "minimum": 0.0,
      "title": "Temperature",
      "type": "number",
      "ui:widget": "range"
    }
  },
  "required": [
    "llm_provider_id",
    "llm_provider_model_id"
  ],
  "title": "LLMResponse",
  "type": "object",
  "ui:can_add": false,
  "ui:can_delete": true,
  "ui:deprecated": true,
  "ui:deprecation_message": "Use the 'LLM response with prompt' node instead.",
  "ui:flow_node_type": "pipelineNode",
  "ui:label": "LLM response"
}